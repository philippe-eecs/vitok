{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "import os\n",
    "import torch, os\n",
    "from diffusers import AutoencoderKL\n",
    "import time, math, torch, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.image import FrechetInceptionDistance as FID, InceptionScore as IS, StructuralSimilarityIndexMeasure as SSIM, PeakSignalNoiseRatio as PSNR\n",
    "from torchmetrics import MeanSquaredError\n",
    "from contextlib import nullcontext    \n",
    "\n",
    "VAE_CHOICES = [\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    \"stabilityai/stable-diffusion-3.5-large\",\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    \"cosmos/Cosmos-Tokenizer-CI16x16\",\n",
    "    \"cosmos/Cosmos-Tokenizer-CI8x8\"\n",
    "\n",
    "]\n",
    "COMPILE_MODEL = False #Generally bugged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from cosmos_tokenizer.image_lib import ImageTokenizer      # comes from Cosmos-Tokenizer\n",
    "import os, torch\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1 · tiny loader for CI-16×16 (≈64 C, 16×16 grid)\n",
    "# -----------------------------------------------------------\n",
    "def load_cosmos_ci16x16(device=\"cuda\",\n",
    "                        dtype=torch.bfloat16,\n",
    "                        cache_dir=\"pretrained_ckpts\"):\n",
    "    model_name   = \"Cosmos-Tokenizer-CI8x8\"\n",
    "    ckpt_dir     = os.path.join(cache_dir, model_name)\n",
    "    \n",
    "    # ––– one-time download (≈325 MB) ––––––––––––––––––––––––\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        snapshot_download(repo_id=f\"nvidia/{model_name}\",\n",
    "                          local_dir=ckpt_dir,\n",
    "                          local_dir_use_symlinks=False)\n",
    "    \n",
    "    enc = ImageTokenizer(checkpoint_enc=f\"{ckpt_dir}/encoder.jit\") \\\n",
    "              .to(device).to(dtype)\n",
    "    dec = ImageTokenizer(checkpoint_dec=f\"{ckpt_dir}/decoder.jit\") \\\n",
    "              .to(device).to(dtype)\n",
    "    \n",
    "    # match your original “vae_encode / vae_decode” API\n",
    "    encode = lambda x: enc.encode(x)[0]      # returns latent tensor\n",
    "    decode = lambda z: dec.decode(z)         # reconstructs image\n",
    "    return encode, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_COSMOS = True          # ⇠ flip this flag\n",
    "\n",
    "if USE_COSMOS:\n",
    "    VAE_CHOICE = 4\n",
    "    vae_encode, vae_decode = load_cosmos_ci16x16()\n",
    "else:\n",
    "    VAE_CHOICE = 2\n",
    "    # ---- your original AutoencoderKL branch ----\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        VAE_CHOICES[VAE_CHOICE],\n",
    "        subfolder=\"vae\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    ).to(\"cuda\")\n",
    "    if COMPILE_MODEL:\n",
    "        vae_encode = torch.compile(vae.encode)\n",
    "        vae_decode = torch.compile(vae.decode)\n",
    "    else:\n",
    "        vae_encode, vae_decode = vae.encode, vae.decode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SIDE_RESOLUTION = 1024\n",
    "\n",
    "def resize_long_side(pil: Image.Image, max_side: int = 1024) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Rescales an image so that its longer edge becomes `max_side`, preserving aspect ratio,\n",
    "    but only if the image's longer edge is greater than `max_side`.\n",
    "    Otherwise, returns the image unchanged.\n",
    "    \"\"\"\n",
    "    w, h = pil.size  # PIL gives (W, H)\n",
    "    long_side = max(w, h)\n",
    "    scale = max_side / long_side\n",
    "    new_w, new_h = int(round(w * scale)), int(round(h * scale))\n",
    "    return pil.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "\n",
    "def pad(t: torch.Tensor, pad_size: int):\n",
    "    _, h, w = t.shape\n",
    "    pad_w = (pad_size - w % pad_size) % pad_size\n",
    "    pad_h = (pad_size - h % pad_size) % pad_size\n",
    "    return F.pad(t, (0, pad_w, 0, pad_h), \"replicate\"), h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure your VAE is loaded\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 0 · config\n",
    "# ---------------------------------------------------------------------\n",
    "src_dir = Path(\"processed\")\n",
    "vae_choice_str = VAE_CHOICES[VAE_CHOICE].replace(\"/\", \"_\")\n",
    "out_dir = Path(f\"decoded_images_{vae_choice_str}\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1 · helper: load VAE\n",
    "# ---------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "# 2 · metrics\n",
    "# ---------------------------------------------------------------------\n",
    "device = \"cuda\"\n",
    "ssim_metric = SSIM(data_range=1.0).to(device)\n",
    "psnr_metric = PSNR(data_range=1.0).to(device)\n",
    "mse_metric = MeanSquaredError().to(device)\n",
    "\n",
    "def encode_decode_paths(paths):\n",
    "    results, stats = {}, []\n",
    "    for p in paths:\n",
    "        pil = Image.open(p).convert(\"RGB\")\n",
    "        # if image size is greater than max_side, resize it\n",
    "        # Only resize if either side is greater than MAX_SIDE_RESOLUTION\n",
    "        ref = TF.to_tensor(pil)\n",
    "        if USE_COSMOS:\n",
    "            refp, H, W = pad(ref, 16)  # multiple of 8\n",
    "        else:\n",
    "            refp, H, W = pad(ref, 8)  # multiple of 8\n",
    "        refp = refp.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad(), torch.amp.autocast(dtype=torch.bfloat16, device_type=device):\n",
    "            ref_scaled = refp.mul(2).sub(1)\n",
    "            if USE_COSMOS:\n",
    "                z = vae_encode(ref_scaled)\n",
    "                print(z.shape)\n",
    "            else:\n",
    "                z = vae_encode(ref_scaled).latent_dist.sample()\n",
    "            if USE_COSMOS:\n",
    "                recon = vae_decode(z)\n",
    "            else:\n",
    "                recon = vae_decode(z).sample\n",
    "            recon = recon.to(torch.float32)[:, :, :H, :W].contiguous()\n",
    "\n",
    "        # quality\n",
    "        recon_01 = recon[:,:,:H,:W].add(1).div(2)\n",
    "        ref_01 = refp[:,:, :H,:W]\n",
    "        ssim = ssim_metric(recon_01, ref_01).item()\n",
    "        psnr = psnr_metric(recon_01, ref_01).item()\n",
    "        # --- FIX: use .reshape(-1) instead of .view(-1) in torchmetrics workaround ---\n",
    "        # This avoids the RuntimeError about non-contiguous tensors.\n",
    "        # See: https://github.com/Lightning-AI/torchmetrics/issues/1862\n",
    "        # and error message: \"view size is not compatible with input tensor's size and stride...\"\n",
    "        # The error is inside torchmetrics, but we can work around it by passing contiguous tensors.\n",
    "        # So, ensure recon_01 and ref_01 are contiguous before passing to mse_metric.\n",
    "        recon_01_contig = recon_01.contiguous()\n",
    "        ref_01_contig = ref_01.contiguous()\n",
    "        rmse = mse_metric(recon_01_contig, ref_01_contig).item() ** 0.5\n",
    "\n",
    "        print(f\"{p.name:22s}  SSIM {ssim:.4f}  PSNR {psnr:.2f} dB   RMSE {rmse:.6f}\")\n",
    "\n",
    "        results[p] = recon.cpu()\n",
    "        stats.append(dict(\n",
    "            file=p.name, width=W, height=H,\n",
    "            ssim=ssim, psnr_db=psnr, rmse=rmse,\n",
    "        ))\n",
    "\n",
    "        del ref, ref_scaled, z, recon\n",
    "        torch.cuda.empty_cache()\n",
    "    return results, stats\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4 · run the batch\n",
    "# ---------------------------------------------------------------------\n",
    "paths = sorted(src_dir.glob(\"*.[jp][pn]g\"))\n",
    "if not paths:\n",
    "    raise RuntimeError(f\"No PNG/JPG files in {src_dir.resolve()}\")\n",
    "\n",
    "recon_imgs, stats = encode_decode_paths(paths)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5 · save & optionally preview\n",
    "# ---------------------------------------------------------------------\n",
    "for idx, path in enumerate(paths):\n",
    "    rec = recon_imgs[path]\n",
    "    img = ((rec.clamp(-1, 1) + 1) / 2).permute(0, 2, 3, 1).cpu().numpy()[0]\n",
    "    img8 = (img * 255).round().astype(\"uint8\")\n",
    "    save_path = out_dir / path.name\n",
    "    Image.fromarray(img8).save(save_path)\n",
    "    print(\"✓ saved\", save_path)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6 · write metrics CSV\n",
    "# ---------------------------------------------------------------------\n",
    "csv_path = out_dir / \"metrics.csv\"\n",
    "pd.DataFrame(stats).to_csv(csv_path, index=False)\n",
    "print(\"✓ metrics written to\", csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "import functools\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MAX_SIDE_RESOLUTION = 1024\n",
    "\n",
    "batch_size = {256: 64, 512: 16, 1024: 16}[MAX_SIDE_RESOLUTION]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(functools.partial(resize_long_side, max_side=MAX_SIDE_RESOLUTION)),\n",
    "    transforms.ToTensor(),                # → [0, 1] float32 CHW\n",
    "    transforms.Lambda(lambda t: t.mul(2).sub(1)),  # scale → [-1, 1]\n",
    "])\n",
    "\n",
    "class ARImageFolder(ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        image, label = super().__getitem__(index)  # pil passes through transform\n",
    "        orig_size = image.shape[1:]\n",
    "        return image, label, orig_size\n",
    "\n",
    "dataset = ARImageFolder(root='/home/ubuntu/imagenet2012/val', transform=transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, labels, orig_hw = zip(*batch)\n",
    "\n",
    "    # Always pad to fixed resolution (MAX_SIDE_RESOLUTION)\n",
    "    target_h = MAX_SIDE_RESOLUTION\n",
    "    target_w = MAX_SIDE_RESOLUTION\n",
    "\n",
    "    imgs_pad = [\n",
    "        F.pad(\n",
    "            t,\n",
    "            (0, target_w - t.shape[2], 0, target_h - t.shape[1]),\n",
    "            \"replicate\"\n",
    "        )\n",
    "        for t in imgs\n",
    "    ]\n",
    "    imgs_pad = torch.stack(imgs_pad, dim=0)      # B × C × H × W\n",
    "    labels   = torch.as_tensor(labels, dtype=torch.long)\n",
    "    return imgs_pad, labels, orig_hw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True,\n",
    "    collate_fn=collate_fn      # <- key change\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "from torchmetrics import MeanSquaredError\n",
    "#from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "#from torchmetrics.image.inception import InceptionScore\n",
    "\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)   # expects [0,1]\n",
    "psnr_metric = PeakSignalNoiseRatio(data_range=1.0).to(device)\n",
    "mse_metric  = MeanSquaredError().to(device)\n",
    "\n",
    "\n",
    "#FID is really slow and will get effected hard by gray padding if we do a batch based version, also bad at high resolutions...\n",
    "#fid_metric  = FrechetInceptionDistance(feature=2048, normalize=True)\n",
    "#is_metric   = InceptionScore(feature=2048, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics = False #If you just want to measure metrics, set this to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch one example\n",
    "# Fetch one batch from the loader and display the first sample\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "vae_choice_str = VAE_CHOICES[VAE_CHOICE].replace(\"/\", \"_\")\n",
    "out_dir = Path(f\"decoded_images_{vae_choice_str}\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "show_batches = 0\n",
    "\n",
    "batch_idx = 0\n",
    "device = 'cuda'\n",
    "ssim_vals = []\n",
    "psnr_vals = []\n",
    "rmse_vals = []\n",
    "total_ms = 0\n",
    "\n",
    "# Get batch size from loader\n",
    "try:\n",
    "    batch_size = loader.batch_size\n",
    "except AttributeError:\n",
    "    # fallback: get from first batch\n",
    "    batch_size = None\n",
    "\n",
    "# Use tqdm with custom description and dynamic postfix\n",
    "with tqdm(loader, desc=\"Evaluating\", dynamic_ncols=True) as pbar:\n",
    "    for imgs_pad, labels, orig_hw in pbar:\n",
    "        imgs_pad = imgs_pad.to(device, non_blocking=True)  # already [-1,1]\n",
    "\n",
    "        time_start = time.time()\n",
    "        with torch.no_grad(), torch.amp.autocast(dtype=torch.bfloat16, device_type=device):\n",
    "            if USE_COSMOS:\n",
    "                z      = vae_encode(imgs_pad)\n",
    "            else:\n",
    "                z      = vae_encode(imgs_pad).latent_dist.sample()\n",
    "            if batch_idx == 0:\n",
    "                print(z.shape)\n",
    "            if USE_COSMOS:\n",
    "                recon  = vae_decode(z)\n",
    "            else:\n",
    "                recon  = vae_decode(z).sample.to(torch.float32)   # still padded\n",
    "        time_end = time.time()\n",
    "        total_ms += (time_end - time_start) * 1000\n",
    "\n",
    "        # --- crop both input & recon to original H,W -------------------\n",
    "        ref01   = imgs_pad.mul(0.5).add(0.5).clamp(0, 1)#.unsqueeze(0)\n",
    "        recon01 = recon.mul(0.5).add(0.5).clamp(0, 1)#.unsqueeze(0)\n",
    "        if compute_metrics:\n",
    "            for i in range(imgs_pad.shape[0]):\n",
    "                # Get original size\n",
    "                orig_h, orig_w = orig_hw[i]\n",
    "                # Get recon shape\n",
    "                recon_h, recon_w = recon01.shape[2], recon01.shape[3]\n",
    "                # For this sample, get the actual recon shape (in case recon is batched)\n",
    "                recon_i_h = recon01[i].shape[1]\n",
    "                recon_i_w = recon01[i].shape[2]\n",
    "                # Compute minimum height and width to avoid shape mismatch\n",
    "                crop_h = min(orig_h, recon01[i].shape[1])\n",
    "                crop_w = min(orig_w, recon01[i].shape[2])\n",
    "                if crop_h != orig_h or crop_w != orig_w:\n",
    "                    print(f\"orig_h: {orig_h}, orig_w: {orig_w}\")\n",
    "                    print(f\"recon_h: {recon_h}, recon_w: {recon_w}\")\n",
    "                    print(f\"recon_i_h: {recon_i_h}, recon_i_w: {recon_i_w}\")\n",
    "                    print(f\"Cropping to {crop_h}x{crop_w}\")\n",
    "\n",
    "                # Crop both tensors to the same minimum size\n",
    "                ref01_i = ref01[i:i+1, :, :crop_h, :crop_w]\n",
    "                recon01_i = recon01[i:i+1, :, :crop_h, :crop_w]\n",
    "\n",
    "                # Ensure tensors are contiguous to avoid .view() issues in torchmetrics\n",
    "                ref01_i = ref01_i.contiguous()\n",
    "                recon01_i = recon01_i.contiguous()\n",
    "\n",
    "                # SSIM / PSNR / RMSE -------------------------------------------\n",
    "                ssim_vals.append(\n",
    "                    ssim_metric(recon01_i, ref01_i).item()\n",
    "                )\n",
    "                psnr_vals.append(\n",
    "                    psnr_metric(recon01_i, ref01_i).item()\n",
    "                )\n",
    "                rmse_vals.append(\n",
    "                    (mse_metric(recon01_i, ref01_i).item()) ** 0.5\n",
    "                )\n",
    "        batch_idx += 1\n",
    "\n",
    "        # Calculate running means and ms per image\n",
    "        n_images = batch_idx * (batch_size if batch_size is not None else imgs_pad.shape[0])\n",
    "        mean_ssim = float(np.mean(ssim_vals)) if ssim_vals else 0.0\n",
    "        mean_psnr = float(np.mean(psnr_vals)) if psnr_vals else 0.0\n",
    "        mean_rmse = float(np.mean(rmse_vals)) if rmse_vals else 0.0\n",
    "        ms_per_image = float(total_ms / n_images) if n_images > 0 else 0.0\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"SSIM\": f\"{mean_ssim:.4f}\",\n",
    "            \"PSNR\": f\"{mean_psnr:.2f}\",\n",
    "            \"RMSE\": f\"{mean_rmse:.4f}\",\n",
    "            \"ms/img\": f\"{ms_per_image:.2f}\"\n",
    "        })\n",
    "\n",
    "# Compute metrics\n",
    "mean_ssim = float(np.mean(ssim_vals))\n",
    "mean_psnr = float(np.mean(psnr_vals))\n",
    "mean_rmse = float(np.mean(rmse_vals))\n",
    "# Use total number of images for final ms per image\n",
    "total_images = batch_idx * (batch_size if batch_size is not None else 1)\n",
    "time_per_image = float(total_ms / total_images) if total_images > 0 else 0.0\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"SSIM: {mean_ssim}\")\n",
    "print(f\"PSNR: {mean_psnr}\")\n",
    "print(f\"RMSE: {mean_rmse}\")\n",
    "print(f\"Time per image: {time_per_image} ms\")\n",
    "\n",
    "# Save metrics to imagenet_metrics.csv in the respective folder\n",
    "# Try to infer the output folder from dataset or loader, fallback to current dir\n",
    "csv_path = os.path.join(out_dir, f\"{MAX_SIDE_RESOLUTION}_imagenet_metrics.csv\")\n",
    "\n",
    "csv_fields = [\"SSIM\", \"PSNR\", \"RMSE\", \"TimePerImage_ms\"]\n",
    "csv_values = [mean_ssim, mean_psnr, mean_rmse, time_per_image]\n",
    "\n",
    "# If file exists, append; else, write header\n",
    "write_header = not os.path.exists(csv_path)\n",
    "try:\n",
    "    with open(csv_path, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if write_header:\n",
    "            writer.writerow(csv_fields)\n",
    "        writer.writerow(csv_values)\n",
    "    print(f\"Metrics saved to {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save metrics to {csv_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
