{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "from utils import get_config, load_model, PatchImageProcessor, remove_center_padding\n",
    "from big_vision.models.proj.vitok.naflex_vit_vae import patches_to_image\n",
    "import jax\n",
    "from jax2torch import jax2torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import jax.numpy as jnp\n",
    "import os\n",
    "\n",
    "os.environ.setdefault(\n",
    "    \"XLA_FLAGS\",\n",
    "   # \"--xla_gpu_enable_async_collectives=true \"\n",
    "    \"--xla_gpu_enable_triton_gemm=true \"\n",
    "    \"--xla_gpu_use_runtime_fusion=true\",\n",
    ")\n",
    "\n",
    "# Constants\n",
    "MAX_SIDE_RESOLUTION = 1024\n",
    "SAVE_VARIANT = 'S_B/16x64' # 'S_B/16x32' or 'S_B/24x64'\n",
    "patch_size = int(SAVE_VARIANT.split('/')[1].split('x')[0])\n",
    "GCS_PATH = {'S_B/16x64':  \"gs://vidtok-data/vae_10/S_B_high_res_finetune/params.npz\", 'S_B/16x32': \"gs://vidtok-data/vae_17/4096_S_B_16_32/params.npz\", \"S_B/24x64\": \"gs://vidtok-data/vae_18/1600_S_B_24_64rr/params.npz\",\n",
    "            \"S_B/16x16+256_fixed_AR\": \"gs://vidtok-data/final/S_B_16_fixed_AR_256_params.npz\",\n",
    "            \"S_B/16x32+256\": \"gs://vidtok-data/final/S_B_16_32_256_params.npz\",\n",
    "            \"S_B/24x64+1800\": \"gs://vidtok-data/final/S_B_24_64_1800_params.npz\",\n",
    "            }[SAVE_VARIANT]\n",
    "MAX_TOKENS = (np.ceil(MAX_SIDE_RESOLUTION / patch_size)) ** 2\n",
    "MAX_TOKENS = int(np.ceil(MAX_TOKENS))\n",
    "print(MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "torch.set_float32_matmul_precision(\"high\")  # TF‑32 matmuls\n",
    "cudnn.allow_tf32 = True                     # TF‑32 convs\n",
    "cudnn.benchmark  = True\n",
    "DTYPE  = torch.bfloat16\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaFlexImageFolder(ImageFolder):\n",
    "    def __init__(self, *args, patch_size=16, max_tokens=1024, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.patch_processor = PatchImageProcessor(patch_size=patch_size, token_match=True, max_tokens=max_tokens)\n",
    "    def __getitem__(self, index):\n",
    "        image, _ = super().__getitem__(index)\n",
    "        return self.patch_processor.preprocess_pil(image)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANT =  SAVE_VARIANT.split('+')[0]\n",
    "config = get_config(f\"variant={VARIANT}\")\n",
    "patch_size = config.patch_size\n",
    "max_grid_size = config.max_grid_size\n",
    "print(max_grid_size)\n",
    "model, params = load_model(config, checkpoint_path=GCS_PATH, max_sequence_len=MAX_TOKENS, patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def recon_apply(batch):\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1].shape)\n",
    "    recon_tuple, _ = model.apply({'params': params}, batch, None)\n",
    "    recon = patches_to_image(recon_tuple, max_grid_size, max_grid_size, patch_size)\n",
    "    reference = patches_to_image(batch, max_grid_size, max_grid_size, patch_size)\n",
    "    return recon, reference\n",
    "\n",
    "forward_torch = jax2torch(recon_apply)\n",
    "#forward_torch = torch.cuda._graph_callable(forward_torch)\n",
    "#forward_torch = torch.compile(forward_torch)\n",
    "\n",
    "def resize_long_side(pil: Image.Image) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Rescales *any* image (up- or down-sampling) so that its longer edge\n",
    "    becomes `max_side`, preserving aspect ratio.\n",
    "    \"\"\"\n",
    "    w, h = pil.size                     # PIL gives (W, H)\n",
    "    scale = MAX_SIDE_RESOLUTION / max(w, h)        # ≥ 1  → upsample,  < 1 → downsample\n",
    "    new_w, new_h = int(round(w * scale)), int(round(h * scale))\n",
    "    if (new_w, new_h) == (w, h):        # already correct\n",
    "        return pil\n",
    "    return pil.resize((new_w, new_h), Image.LANCZOS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import time\n",
    "from PIL import Image\n",
    "from torchmetrics.image import FrechetInceptionDistance as FID, InceptionScore as IS, StructuralSimilarityIndexMeasure as SSIM, PeakSignalNoiseRatio as PSNR\n",
    "from torchmetrics import MeanSquaredError as MSE\n",
    "import glob\n",
    "\n",
    "patch_processor = PatchImageProcessor(patch_size=patch_size)\n",
    "\n",
    "# ────────── 4. metrics & misc  ──────────────────────────────────────\n",
    "ssim_metric = SSIM(data_range=1.0).to('cuda')\n",
    "psnr_metric = PSNR(data_range=1.0).to('cuda')\n",
    "mse_metric  = MSE().to('cuda')\n",
    "\n",
    "def to_uint8(chw):                # torch.float in [0,1]\n",
    "    return (chw.clamp(0,1)*255).byte().cpu().permute(1,2,0).numpy()\n",
    "\n",
    "# ────────── 5. main loop ───────────────────────────────────────────\n",
    "SAVE_DIR = Path(f\"decoded_images_{SAVE_VARIANT}\".replace(\"/\", \"_\")); SAVE_DIR.mkdir(exist_ok=True)\n",
    "stats = []\n",
    "\n",
    "for path in sorted(glob.glob(\"processed/*.[jp][pn]g\")):\n",
    "    img_name = Path(path).stem\n",
    "    pil      = Image.open(path).convert(\"RGB\")\n",
    "    orig_wh  = pil.size\n",
    "    print(orig_wh)\n",
    "    orig_hw  = orig_wh[1], orig_wh[0]\n",
    "    patches, ptype, yidx, xidx, new_orig_hw = patch_processor.preprocess_pil(pil)[:5]\n",
    "    print(new_orig_hw)\n",
    "    if new_orig_hw != orig_hw:\n",
    "        print(\"Original size mismatch\")\n",
    "        print(orig_hw)\n",
    "        print(new_orig_hw)\n",
    "        assert False\n",
    "\n",
    "    batch = (\n",
    "        patches.to('cuda').contiguous().unsqueeze(0).repeat(2,1,1), #Need to repeat due to issue with batch in Jax vs PyTorch\n",
    "        ptype.to('cuda').contiguous().repeat(2,1),\n",
    "        yidx.to('cuda').contiguous().repeat(2,1),\n",
    "        xidx.to('cuda').contiguous().repeat(2,1),\n",
    "    )\n",
    "\n",
    "    not_gray = (ptype != 0)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    recon, ref = forward_torch(batch)      # ⬅ no XlaRuntimeError now\n",
    "    ms = (time.perf_counter() - t0)*1000\n",
    "\n",
    "    max_y = torch.where(not_gray, yidx, torch.full_like(yidx, -1)).max().item()\n",
    "    max_x = torch.where(not_gray, xidx, torch.full_like(xidx, -1)).max().item()\n",
    "    row_end = (max_y + 1) * patch_size\n",
    "    col_end = (max_x + 1) * patch_size\n",
    "    # Crop to valid region first\n",
    "    ref_img = ref[:, :row_end, :col_end, :][0]\n",
    "    recon_img = recon[:, :row_end, :col_end, :][0]\n",
    "\n",
    "    print(row_end, col_end)\n",
    "    # Then remove center padding to get original size\n",
    "    ref_final = remove_center_padding(ref_img, (int(orig_hw[0]), int(orig_hw[1])))\n",
    "    recon_final = remove_center_padding(recon_img, (int(orig_hw[0]), int(orig_hw[1])))\n",
    "\n",
    "    print(orig_hw)\n",
    "    print(ref_final.shape, recon_final.shape)\n",
    "    print(ref_final.min(), ref_final.max(), recon_final.min(), recon_final.max())\n",
    "\n",
    "    ref_final = ref_final.permute(2, 0, 1).unsqueeze(0).add(1).div(2)\n",
    "    recon_final = recon_final.permute(2, 0, 1).unsqueeze(0).add(1).div(2)\n",
    "\n",
    "    print(ref_final.shape, recon_final.shape)\n",
    "\n",
    "    # Ensure tensors are contiguous before passing to metrics\n",
    "    ref_final = ref_final.contiguous()\n",
    "    recon_final = recon_final.contiguous()\n",
    "\n",
    "    # SSIM / PSNR / RMSE -------------------------------------------\n",
    "    ssim_val = ssim_metric(recon_final, ref_final).item()\n",
    "    psnr_val = psnr_metric(recon_final, ref_final).item()\n",
    "    rmse_val = mse_metric(recon_final, ref_final).item() ** 0.5\n",
    "\n",
    "    # Convert to uint8 and remove batch dimension for saving\n",
    "    recon_final = (recon_final * 255).to(torch.uint8)[0]\n",
    "    ref_final = (ref_final * 255).to(torch.uint8)[0]\n",
    "\n",
    "    # Convert from CHW to HWC for PIL\n",
    "    recon_final_np = recon_final.permute(1, 2, 0).cpu().numpy()\n",
    "    ref_final_np = ref_final.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Save images as PNG\n",
    "    Image.fromarray(recon_final_np).save(SAVE_DIR/f\"{img_name}.png\")\n",
    "    Image.fromarray(ref_final_np).save(SAVE_DIR/f\"{img_name}_ref.png\")\n",
    "\n",
    "    print(f\"{img_name:20s}  SSIM {ssim_val:.4f}  PSNR {psnr_val:.2f} dB  RMSE {rmse_val:.6f}  {ms:6.1f} ms\")\n",
    "\n",
    "    stats.append(dict(file=Path(path).name,\n",
    "                      height=orig_hw[0], width=orig_hw[1],\n",
    "                      ssim=ssim_val, psnr_db=psnr_val, rmse=rmse_val,\n",
    "                      elapsed_ms=ms))\n",
    "\n",
    "# ────────── 6. CSV ─────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "pd.DataFrame(stats).to_csv(SAVE_DIR/\"metrics.csv\", index=False)\n",
    "print(\"✓ metrics written:\", SAVE_DIR/\"metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SIDE_RESOLUTION = 2048\n",
    "MAX_TOKENS = (np.ceil(MAX_SIDE_RESOLUTION / patch_size)) ** 2\n",
    "MAX_TOKENS = int(np.ceil(MAX_TOKENS))\n",
    "print(MAX_TOKENS)\n",
    "batch_size = {256: 512, 512: 128, 1024: 32, 2048: 1}[MAX_SIDE_RESOLUTION]\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(resize_long_side),\n",
    "])\n",
    "DATASET = 'div8k' #'imagenet2012' or 'div8k'\n",
    "if DATASET == 'imagenet2012':\n",
    "    path = '/home/ubuntu/imagenet2012/val'\n",
    "elif DATASET == 'div8k':\n",
    "    path = '/home/ubuntu/datasets/div8k/TestSets/test8k'\n",
    "dataset = NaFlexImageFolder(root=path, transform=transform, patch_size=patch_size, max_tokens=MAX_TOKENS) #Pad an extra token in case\n",
    "loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "from torchmetrics import MeanSquaredError\n",
    "#from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "#from torchmetrics.image.inception import InceptionScore\n",
    "device = 'cuda'\n",
    "\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)   # expects [0,1]\n",
    "psnr_metric = PeakSignalNoiseRatio(data_range=1.0).to(device)\n",
    "mse_metric  = MeanSquaredError().to(device)\n",
    "\n",
    "\n",
    "#FID is really slow and will get effected hard by gray padding if we do a batch based version, also bad at high resolutions...\n",
    "#fid_metric  = FrechetInceptionDistance(feature=2048, normalize=True)\n",
    "#is_metric   = InceptionScore(feature=2048, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics = True #If you just want to measure metrics, set this to False. The time measurement gets messed up otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch one example\n",
    "# Fetch one batch from the loader and display the first sample\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import csv\n",
    "\n",
    "out_dir = Path(f\"decoded_images_{SAVE_VARIANT}\".replace(\"/\", \"_\"))\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "device = 'cuda'\n",
    "ssim_vals = []\n",
    "psnr_vals = []\n",
    "rmse_vals = []\n",
    "total_ms = 0\n",
    "num_examples = 0\n",
    "\n",
    "# Create tqdm object manually so we can update the description\n",
    "pbar = tqdm(loader, desc='Eval')\n",
    "\n",
    "for patches, ptype, yidx, xidx, orig_hw in pbar:\n",
    "    patches = patches.to('cuda')\n",
    "    ptype = ptype.to('cuda')\n",
    "    yidx = yidx.to('cuda')\n",
    "    xidx = xidx.to('cuda')\n",
    "\n",
    "    time_start = time.time()\n",
    "    with torch.inference_mode(), torch.amp.autocast(dtype=torch.bfloat16, device_type='cuda'):\n",
    "        if batch_size == 1:\n",
    "            batch = (patches.to(torch.bfloat16)[0].contiguous(), ptype[0].contiguous(), yidx[0].contiguous(), xidx[0].contiguous())\n",
    "        recon, ref = forward_torch(batch)\n",
    "    time_end = time.time()\n",
    "    total_ms += (time_end - time_start) * 1000\n",
    "    if compute_metrics:\n",
    "        for i in range(patches.shape[0]):\n",
    "            #crop to original size\n",
    "            not_gray = (ptype[i] != 0)\n",
    "            max_y = torch.where(not_gray, yidx[i], torch.full_like(yidx[i], -1)).max().item()\n",
    "            max_x = torch.where(not_gray, xidx[i], torch.full_like(xidx[i], -1)).max().item()\n",
    "            row_end = (max_y + 1) * patch_size\n",
    "            col_end = (max_x + 1) * patch_size\n",
    "            # Crop to valid region first\n",
    "            ref_img = ref[i][:row_end, :col_end, :]\n",
    "            recon_img = recon[i][:row_end, :col_end, :]\n",
    "            # Then remove center padding to get original size\n",
    "            ref_final = remove_center_padding(ref_img, (int(orig_hw[0][i]), int(orig_hw[1][i])))\n",
    "            recon_final = remove_center_padding(recon_img, (int(orig_hw[0][i]), int(orig_hw[1][i])))\n",
    "\n",
    "            ref_final = ref_final.permute(2, 0, 1).unsqueeze(0).add(1).div(2).to(torch.float32)\n",
    "            recon_final = recon_final.permute(2, 0, 1).unsqueeze(0).add(1).div(2).to(torch.float32)\n",
    "\n",
    "            # Ensure tensors are contiguous before passing to metrics\n",
    "            ref_final = ref_final.contiguous()\n",
    "            recon_final = recon_final.contiguous()\n",
    "\n",
    "            # SSIM / PSNR / RMSE -------------------------------------------\n",
    "            ssim_val = ssim_metric(recon_final, ref_final).item()\n",
    "            psnr_val = psnr_metric(recon_final, ref_final).item()\n",
    "            rmse_val = (mse_metric(recon_final, ref_final).item()) ** 0.5\n",
    "\n",
    "            #save first 4 images for first batch\n",
    "            if i < 4 and num_examples == 0:\n",
    "                recon_final_np = recon_final[0].permute(1, 2, 0).cpu().numpy()\n",
    "                recon_final_np = (recon_final_np * 255).astype(np.uint8)\n",
    "                ref_final_np = ref_final[0].permute(1, 2, 0).cpu().numpy()\n",
    "                ref_final_np = (ref_final_np * 255).astype(np.uint8)\n",
    "                Image.fromarray(recon_final_np).save(SAVE_DIR/f\"recon_{i}.png\")\n",
    "                Image.fromarray(ref_final_np).save(SAVE_DIR/f\"ref_{i}.png\")\n",
    "\n",
    "            # Check for NaN and handle\n",
    "            if np.isnan(ssim_val):\n",
    "                print(f\"NaN detected in SSIM for sample {i}, setting to 0.0\")\n",
    "                ssim_val = 0.0\n",
    "            if np.isnan(psnr_val):\n",
    "                print(f\"NaN detected in PSNR for sample {i}, setting to 0.0\")\n",
    "                psnr_val = 0.0\n",
    "            if np.isnan(rmse_val):\n",
    "                print(f\"NaN detected in RMSE for sample {i}, setting to 0.0\")\n",
    "                rmse_val = 0.0\n",
    "\n",
    "            ssim_vals.append(ssim_val)\n",
    "            psnr_vals.append(psnr_val)\n",
    "            rmse_vals.append(rmse_val)\n",
    "    else:\n",
    "        for i in range(patches.shape[0]):\n",
    "            ssim_vals.append(0.0)\n",
    "            psnr_vals.append(0.0)\n",
    "            rmse_vals.append(0.0)\n",
    "    \n",
    "    num_examples += patches.shape[0]\n",
    "\n",
    "    # Update tqdm with current mean metrics\n",
    "    if len(ssim_vals) > 0:\n",
    "        mean_ssim = float(np.mean(ssim_vals))\n",
    "        mean_psnr = float(np.mean(psnr_vals))\n",
    "        mean_rmse = float(np.mean(rmse_vals))\n",
    "        time_per_image = float(total_ms / num_examples)\n",
    "        pbar.set_postfix({\n",
    "            \"SSIM\": f\"{mean_ssim:.4f}\",\n",
    "            \"PSNR\": f\"{mean_psnr:.2f}\",\n",
    "            \"RMSE\": f\"{mean_rmse:.4f}\",\n",
    "            \"TimePerImage_ms\": f\"{time_per_image:.2f}\"\n",
    "        })\n",
    "\n",
    "# Compute metrics\n",
    "mean_ssim = float(np.mean(ssim_vals))\n",
    "mean_psnr = float(np.mean(psnr_vals))\n",
    "mean_rmse = float(np.mean(rmse_vals))\n",
    "time_per_image = float(total_ms / len(loader))\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"SSIM: {mean_ssim}\")\n",
    "print(f\"PSNR: {mean_psnr}\")\n",
    "print(f\"RMSE: {mean_rmse}\")\n",
    "print(f\"Time per image: {time_per_image} ms\")\n",
    "\n",
    "# Save metrics to imagenet_metrics.csv in the respective folder\n",
    "# Try to infer the output folder from dataset or loader, fallback to current dir\n",
    "csv_path = os.path.join(out_dir, f\"{MAX_SIDE_RESOLUTION}_{DATASET}_metrics.csv\")\n",
    "\n",
    "csv_fields = [\"SSIM\", \"PSNR\", \"RMSE\", \"TimePerImage_ms\"]\n",
    "csv_values = [mean_ssim, mean_psnr, mean_rmse, time_per_image]\n",
    "\n",
    "# If file exists, append; else, write header\n",
    "write_header = not os.path.exists(csv_path)\n",
    "try:\n",
    "    with open(csv_path, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if write_header:\n",
    "            writer.writerow(csv_fields)\n",
    "        writer.writerow(csv_values)\n",
    "    print(f\"Metrics saved to {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save metrics to {csv_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VAE FIGURE GENERATOR – RMSE fixed‑v‑native bar charts (per‑run aggregation)\n",
    "NeurIPS‑ready (v2: bigger, clearer, no clipping)\n",
    "──────────────────────────────────────────────────────────────────────────────\n",
    "* Enlarged base font, thicker bars, higher DPI, wider figure.\n",
    "* Tick labels wrapped onto two lines to avoid clipping.\n",
    "* Legend anchored below plot; no in‑plot title.\n",
    "* All label‑swap logic and data handling remain unchanged.\n",
    "\"\"\"\n",
    "\n",
    "# ── 0 · Global style (LaTeX OFF) ────────────────────────────────\n",
    "import matplotlib as mpl, matplotlib.pyplot as plt\n",
    "BASE_FONTSIZE = 16  # bigger for camera‑ready\n",
    "mpl.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.size\": BASE_FONTSIZE,\n",
    "    \"axes.labelsize\": BASE_FONTSIZE + 1,\n",
    "    \"axes.titlesize\": BASE_FONTSIZE + 1,\n",
    "    \"xtick.labelsize\": BASE_FONTSIZE - 1,\n",
    "    \"ytick.labelsize\": BASE_FONTSIZE - 1,\n",
    "    \"legend.fontsize\": BASE_FONTSIZE - 1,\n",
    "    \"figure.dpi\": 180,            # higher resolution\n",
    "    \"figure.constrained_layout.use\": True,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.linewidth\": 0.7,\n",
    "    \"xtick.direction\": \"out\",\n",
    "    \"ytick.direction\": \"out\",\n",
    "    \"grid.alpha\": 0.35,\n",
    "})\n",
    "\n",
    "SHOW_FIGS = True  # toggle windows when running interactively\n",
    "\n",
    "# ── 1 · Configuration ──────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "import os, re, numpy as np, pandas as pd\n",
    "\n",
    "ROOT = Path(\".\")  # directory with decoded_images_* folders\n",
    "\n",
    "RUNS = [\n",
    "    \"S_B_16x32+256_fixedAR\",\n",
    "    \"S_B_16x32+256\",\n",
    "    \"S_B_16x32\",\n",
    "    \"S_B_16x64\",\n",
    "    \"Cosmos-Tokenizer-CI8x8\",\n",
    "]\n",
    "\n",
    "RUN_LABELS = {\n",
    "    \"S_B_16x32+256_fixedAR\": \"256p\\nFixed 32c\",\n",
    "    \"S_B_16x32+256\": \"256‑Tok\\n32c\",\n",
    "    \"S_B_16x32\": \"4k‑Tok\\n32c\",\n",
    "    \"S_B_16x64\": \"4k‑Tok\\n64c\",\n",
    "    \"Cosmos-Tokenizer-CI8x8\": \"Cosmos 8x8\",\n",
    "}\n",
    "\n",
    "CSV_NAME = \"metrics.csv\"\n",
    "\n",
    "STYLE_COLOURS = {\"box\": \"#1f77b4\", \"ar\": \"#ff7f0e\"}\n",
    "STYLE_ORDER = [\"box\", \"ar\"]\n",
    "STYLE_LABELS = {\"box\": \"Fixed 1:1\", \"ar\": \"Native AR\"}\n",
    "\n",
    "OUT_DIR = Path(\"figs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ── 2 · Helper functions ──────────────────────────────────────\n",
    "_res_pat = re.compile(r\"_(256|512|1024)(?:\\D|$)\")\n",
    "\n",
    "# ── 2 · Helper functions  ───────────────────────────────────────\n",
    "METRICS      = [\"rmse\", \"ssim\", \"psnr_db\"]               # moved up so helpers can see it\n",
    "METRIC_LABEL = {\"rmse\": \"RMSE (↓)\",\n",
    "                \"ssim\": \"SSIM (↑)\",\n",
    "                \"psnr_db\": \"PSNR (↑)\"}\n",
    "\n",
    "_res_pat = re.compile(r\"_(256|512|1024)(?:\\D|$)\")\n",
    "\n",
    "def _parse_res_style(fname: str):\n",
    "    base = os.path.basename(fname)\n",
    "    m = _res_pat.search(base)\n",
    "    res = int(m.group(1)) if m else 1024\n",
    "    style = \"ar\" if \"_ar_\" in base or base.endswith(\"_ar.png\") else \"box\"\n",
    "    if m is None and \"_ar_\" not in base:\n",
    "        style = \"ar\"             # default 1024-AR\n",
    "    return res, style\n",
    "\n",
    "\n",
    "def load_means(run_tag: str):\n",
    "    \"\"\"\n",
    "    Return {(res, style, metric) -> mean value} for *all* metrics in METRICS.\n",
    "    \"\"\"\n",
    "    csv_path = ROOT / f\"decoded_images_{run_tag}\" / CSV_NAME\n",
    "    print(csv_path)\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(csv_path)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(df)\n",
    "    df[[\"res\", \"style\"]] = df[\"file\"].apply(\n",
    "        lambda s: pd.Series(_parse_res_style(s))\n",
    "    )\n",
    "\n",
    "    # Swap labels if this is the mis-labelled run\n",
    "    if \"fixedAR\" in run_tag:\n",
    "        df[\"style\"] = df[\"style\"].map({\"box\": \"ar\", \"ar\": \"box\"})\n",
    "\n",
    "    print(run_tag)\n",
    "\n",
    "    # group means for every metric we care about\n",
    "    grouped = (\n",
    "        df.groupby([\"res\", \"style\"])[METRICS]\n",
    "          .mean()                               # -> MultiIndex rows, columns = metrics\n",
    "    )\n",
    "\n",
    "    # flatten to {(res, style, metric): value}\n",
    "    flat = {}\n",
    "    for (res, sty), row in grouped.iterrows():\n",
    "        print(row)\n",
    "        for metric in METRICS:\n",
    "            flat[(res, sty, metric)] = row[metric]\n",
    "    return flat\n",
    "\n",
    "\n",
    "# ── 3 · Pre-compute per-run means  ──────────────────────────────\n",
    "run_means = {run: load_means(run) for run in RUNS}\n",
    "\n",
    "print(run_means)\n",
    "\n",
    "\n",
    "FIGSIZE = (11.0, 5.8)\n",
    "BAR_W   = 0.18\n",
    "GROUP_OFF = (np.arange(len(STYLE_SEQ)) - 1.5) * BAR_W   # offsets −0.27 … +0.27\n",
    "x = np.arange(len(RUNS))\n",
    "\n",
    "for metric in METRICS:\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "\n",
    "    for idx, (sty, res) in enumerate(STYLE_SEQ):\n",
    "        # fetch values (NaN if missing)\n",
    "        vals = [\n",
    "            run_means[run].get((res, sty, metric), np.nan)\n",
    "            for run in RUNS\n",
    "        ]\n",
    "        ax.bar(\n",
    "            x + GROUP_OFF[idx],\n",
    "            [0 if np.isnan(v) else v for v in vals],\n",
    "            width=BAR_W,\n",
    "            color=STYLE_COLOURS[sty],\n",
    "            alpha=1.0 if res == 1024 else 0.55,\n",
    "            label=f\"{STYLE_LABELS[sty]} {res}p\",\n",
    "        )\n",
    "\n",
    "        # per-bar annotation\n",
    "        for i, v in enumerate(vals):\n",
    "            if metric == \"psnr_db\":\n",
    "                txt = \"–\" if np.isnan(v) else f\"{v:.1f}\"\n",
    "            elif metric == \"ssim\":\n",
    "                txt = \"–\" if np.isnan(v) else f\"{v:.2f}\"\n",
    "            else: #remove initial 0. \n",
    "                txt = \"–\" if np.isnan(v) else f\"{v:.3f}\"\n",
    "                txt = txt[1:]\n",
    "            ax.text(\n",
    "                i + GROUP_OFF[idx],\n",
    "                (v if not np.isnan(v) else 0) + 0.003,\n",
    "                txt,\n",
    "                ha=\"center\", va=\"bottom\",\n",
    "                fontsize=BASE_FONTSIZE - 5,\n",
    "            )\n",
    "\n",
    "    ax.set_xticks(x, [RUN_LABELS[r] for r in RUNS])\n",
    "    ax.set_ylabel(METRIC_LABEL[metric])\n",
    "    ax.grid(axis=\"y\", linestyle=\":\", linewidth=0.4)\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "    ax.legend(\n",
    "        frameon=False, ncol=4, loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, -0.18), columnspacing=1.8,\n",
    "    )\n",
    "\n",
    "    fname = OUT_DIR / f\"{metric}_fixed_vs_native_clustered.pdf\"\n",
    "    fig.savefig(fname, format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "    print(\"✓ Saved\", fname)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"All done – PDFs are in\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#   VAE FIGURE GENERATOR  –  drag-and-drop PDFs for Overleaf\n",
    "# ================================================================\n",
    "# • Mean-only bars, dense bars\n",
    "# • Full-image strip\n",
    "# • Full-image strip with centre-crop inset\n",
    "# • 10 random 384×384 crops (×4 zoom)  + concatenated contact-sheet\n",
    "# • Single centre 384×384 crop (×4 zoom)\n",
    "# ------------------------------------------------\n",
    "#  1.  Edit RUNS / RUN_LABELS / METRICS / ROOT if needed\n",
    "#  2.  Run the script (python or notebook cell)\n",
    "#  3.  All PDFs appear in ./figs/\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "# ── 0 · Global style (LaTeX OFF) ────────────────────────────────\n",
    "import matplotlib as mpl, matplotlib.pyplot as plt\n",
    "mpl.rcParams.update({\n",
    "    \"text.usetex\":       False,\n",
    "    \"font.size\":         9,\n",
    "    \"axes.labelsize\":    9,\n",
    "    \"axes.titlesize\":    9,\n",
    "    \"figure.dpi\":        110,\n",
    "    \"figure.constrained_layout.use\": True,\n",
    "    \"axes.spines.top\":   False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.linewidth\":    0.6,\n",
    "    \"xtick.direction\":   \"out\",\n",
    "    \"ytick.direction\":   \"out\",\n",
    "    \"grid.alpha\":        0.3,\n",
    "})\n",
    "\n",
    "# ── 1 · Configuration ──────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "import os, re, random, numpy as np, pandas as pd\n",
    "from PIL import Image\n",
    "from functools import reduce\n",
    "\n",
    "ROOT   = Path(\".\")    # directory with decoded_images_* folders\n",
    "\n",
    "#Cosmos-Tokenizer-CI8x8\n",
    "\n",
    "RUNS = [\n",
    "    \"Cosmos-Tokenizer-CI8x8\",\n",
    "    \"S_B_16x64\",\n",
    "]\n",
    "\n",
    "RUN_LABELS = {\n",
    "    \"Cosmos-Tokenizer-CI8x8\":             \"Patch Size 8, Channel 8\",\n",
    "    \"S_B_16x64\":         \"Patch Size 16, Channel 32\",\n",
    "}\n",
    "\n",
    "CSV_NAME  = \"metrics.csv\"\n",
    "IMG_NAME  = \"owl_eye_1024.png\"\n",
    "\n",
    "IMG_STRIP_H  = 3.8      # inches – height of full-image strip\n",
    "CROP_PX      = 384\n",
    "N_CROPS      = 10\n",
    "ZOOM_FACTOR  = 4\n",
    "SEED_START   = 1        # seeds = 1 … 10\n",
    "\n",
    "PALETTE = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n",
    "IMG_DIR = Path(\"processed\")\n",
    "OUT_DIR = Path(\"figs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ── 2 · Locate runs & load metrics ─────────────────────────────\n",
    "pat  = re.compile(r\"decoded_images_(.+)\")\n",
    "runs = {m.group(1): ROOT / d for d in os.listdir(ROOT)\n",
    "        if (m := pat.match(d)) and (ROOT / d).is_dir()}\n",
    "missing = set(RUNS) - runs.keys()\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing decoded_images_* folders for: {missing}\")\n",
    "print(\"▶ Using runs:\", \", \".join(RUN_LABELS[t] for t in RUNS))\n",
    "\n",
    "dfs = []\n",
    "for tag in RUNS:\n",
    "    df = pd.read_csv(runs[tag] / CSV_NAME).add_suffix(f\"_{tag}\")\n",
    "    df = df.rename(columns={f\"file_{tag}\": \"file\"})\n",
    "    dfs.append(df)\n",
    "df = reduce(lambda a, b: pd.merge(a, b, on=\"file\", how=\"inner\"), dfs)\n",
    "\n",
    "shape_cols = [c for c in df.columns if c.startswith((\"height_\", \"width_\"))]\n",
    "df = df[df[shape_cols].nunique(axis=1).eq(1)].reset_index(drop=True)\n",
    "print(f\"✓ {len(df)} images with matching resolution\")\n",
    "\n",
    "def crop_strip(x0, y0, crop_px, zoom, title, height_in=4.2):\n",
    "    fig, axes = plt.subplots(1, len(RUNS),\n",
    "                             figsize=(len(RUNS) * 4.0, height_in))\n",
    "    for ax, tag, col in zip(axes, RUNS, PALETTE):\n",
    "        crop = Image.open(runs[tag] / IMG_NAME).crop(\n",
    "            (x0, y0, x0 + crop_px, y0 + crop_px))\n",
    "        crop = crop.resize((crop_px * zoom, crop_px * zoom),\n",
    "                           resample=Image.NEAREST)\n",
    "        ax.imshow(crop)\n",
    "        ax.set_axis_off()\n",
    "        ax.set_title(RUN_LABELS[tag], color=col, fontsize=9)\n",
    "    fig.suptitle(title, y=0.995, fontsize=9)\n",
    "    return fig\n",
    "\n",
    "def random_crop_strip(seed, crop_px=CROP_PX, zoom=ZOOM_FACTOR):\n",
    "    random.seed(seed)\n",
    "    ref = Image.open(runs[RUNS[0]] / IMG_NAME)\n",
    "    W, H = ref.size\n",
    "    x0, y0 = random.randint(0, W - crop_px), random.randint(0, H - crop_px)\n",
    "    title = f\"Random {crop_px}×{crop_px} crop ×{zoom} (seed={seed})\"\n",
    "    return crop_strip(x0, y0, crop_px, zoom, title)\n",
    "\n",
    "def center_crop_strip(crop_px=CROP_PX, zoom=ZOOM_FACTOR):\n",
    "    ref = Image.open(runs[RUNS[0]] / IMG_NAME)\n",
    "    W, H = ref.size\n",
    "    x0, y0 = (W - crop_px) // 2, (H - crop_px) // 2\n",
    "    title = f\"Centre {crop_px}×{crop_px} crop ×{zoom}\"\n",
    "    return crop_strip(x0, y0, crop_px, zoom, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DISPLAY_PX  = 512\n",
    "RNG         = random.Random()\n",
    "\n",
    "def save_fixed_tile_and_full(crop_px=CROP_PX, seed: int | None = None):\n",
    "    \"\"\"\n",
    "    • Draw ONE random crop (seed-controlled) on the reference image.\n",
    "    • Re-use the *exact same* (x0, y0) for every run.\n",
    "    • Produces <tag>_full.png  (with red rectangle)\n",
    "              + <tag>_tile.png  (scaled 384 × 384 tile).\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        RNG.seed(seed)\n",
    "\n",
    "    # ── pick (x0, y0) ONCE from reference ───────────────────────────\n",
    "    ref_img = Image.open(IMG_DIR / IMG_NAME)\n",
    "    W, H    = ref_img.size\n",
    "    if W >= 1024 or H >= 1024:\n",
    "        x0      = RNG.randint(0, W - crop_px)\n",
    "        y0      = RNG.randint(0, H - crop_px)\n",
    "    else:\n",
    "        x0 = 0\n",
    "        y0 = 0\n",
    "        crop_px = 0\n",
    "\n",
    "    def save_full(img, out_path):\n",
    "        fig, ax = plt.subplots(figsize=(4.2, 3.2))\n",
    "        ax.imshow(img); ax.set_axis_off()\n",
    "        if W >= 1024 or H >= 1024:\n",
    "            ax.add_patch(Rectangle((x0, y0), crop_px, crop_px,\n",
    "                                edgecolor=\"red\", linewidth=1.2, facecolor=\"none\"))\n",
    "        fig.savefig(out_path, dpi=300, bbox_inches=\"tight\",\n",
    "                    pad_inches=0, transparent=True)\n",
    "        plt.close(fig)\n",
    "\n",
    "    def save_tile(img, out_path):\n",
    "        crop = img.crop((x0, y0, x0 + crop_px, y0 + crop_px))\n",
    "        crop = crop.resize((DISPLAY_PX, DISPLAY_PX), Image.NEAREST)\n",
    "        crop.save(out_path, format=\"png\")\n",
    "\n",
    "    # ── reference outputs ───────────────────────────────────────────\n",
    "    full_out = OUT_DIR / f\"{IMG_NAME.replace('.png','')}_full.png\"\n",
    "    tile_out = OUT_DIR / f\"{IMG_NAME.replace('.png','')}_tile.png\"\n",
    "    save_full(ref_img, full_out)\n",
    "    save_tile(ref_img, tile_out)\n",
    "    print(\"  ↳ saved\", full_out.name, tile_out.name)\n",
    "\n",
    "    # ── same crop for every run ─────────────────────────────────────\n",
    "    for tag in RUNS:\n",
    "        img       = Image.open(runs[tag] / IMG_NAME)\n",
    "        full_out  = OUT_DIR / f\"{tag}_{IMG_NAME.replace('.png','')}_full.png\"\n",
    "        #only do if image is > 1024\n",
    "        save_full(img, full_out)\n",
    "        if img.size[0] >= 1024 or img.size[1] >= 1024:\n",
    "            tile_out  = OUT_DIR / f\"{tag}_{IMG_NAME.replace('.png','')}_tile.png\"\n",
    "            \n",
    "            save_tile(img, tile_out)\n",
    "        print(\"  ↳ saved\", full_out.name, tile_out.name)\n",
    "\n",
    "# ── call once near the end ─────────────────────────────────────────\n",
    "# Example: save_fixed_tile_and_full(seed=42)  # fixed RNG for full reproducibility\n",
    "save_fixed_tile_and_full()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
